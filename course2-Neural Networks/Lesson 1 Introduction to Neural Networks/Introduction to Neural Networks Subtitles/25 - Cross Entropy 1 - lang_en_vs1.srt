1
00:00:00,000 --> 00:00:02,250
Correct. The answer is logarithm,

2
00:00:02,250 --> 00:00:06,390
because logarithm has this very nice identity that says that the logarithm of

3
00:00:06,390 --> 00:00:11,930
the product A times B is the sum of the logarithms of A and B.

4
00:00:11,930 --> 00:00:13,295
So this is what we do.

5
00:00:13,295 --> 00:00:17,560
We take our product and we take the logarithms,

6
00:00:17,560 --> 00:00:21,855
so now we get a sum of the logarithms of the factors.

7
00:00:21,855 --> 00:00:28,220
So the logarithm of 0.6 times 0.2 times 0.1 times 0.7 is equal to logarithm

8
00:00:28,220 --> 00:00:35,700
0.6 plus logarithm 0.2 plus logarithm 0.1 etc.. Now from now until the end of class,

9
00:00:35,700 --> 00:00:40,040
we'll be taking the natural logarithm which is base E instead of 10.

10
00:00:40,040 --> 00:00:41,760
Nothing different happens with base 10.

11
00:00:41,760 --> 00:00:44,945
Everything works the same as everything gets scaled by the same factor.

12
00:00:44,945 --> 00:00:46,770
So it's just more for convention.

13
00:00:46,770 --> 00:00:51,330
We can calculate those values and get minus 0.51, minus 1.61,

14
00:00:51,330 --> 00:00:58,165
minus 0.23 etc.. Notice that they are all negative numbers and that actually makes sense.

15
00:00:58,165 --> 00:01:01,560
This is because the logarithm of a number between 0 and 1 is always

16
00:01:01,560 --> 00:01:05,595
a negative number since the logarithm of one is zero.

17
00:01:05,595 --> 00:01:07,790
So it actually makes sense to think of the negative of

18
00:01:07,790 --> 00:01:11,260
the logarithm of the probabilities and we'll get positive numbers.

19
00:01:11,260 --> 00:01:15,740
So that's what we'll do. We'll take the negative of the logarithm of the probabilities.

20
00:01:15,740 --> 00:01:18,905
That sum up negatives of logarithms of the probabilities,

21
00:01:18,905 --> 00:01:23,180
we'll call the cross entropy which is a very important concept in the class.

22
00:01:23,180 --> 00:01:25,385
If we calculate the cross entropies,

23
00:01:25,385 --> 00:01:30,255
we see that the bad model on left has a cross entropy 4.8 which is high.

24
00:01:30,255 --> 00:01:35,230
Whereas the good model on the right has a cross entropy of 1.2 which is low.

25
00:01:35,230 --> 00:01:37,455
This actually happens all the time.

26
00:01:37,455 --> 00:01:38,810
A good model will give us

27
00:01:38,810 --> 00:01:43,185
a low cross entropy and a bad model will give us a high cross entropy.

28
00:01:43,185 --> 00:01:44,630
The reason for this is simply that

29
00:01:44,630 --> 00:01:47,390
a good model gives us a high probability and the negative

30
00:01:47,390 --> 00:01:52,600
of the logarithm of a large number is a small number and vice versa.

31
00:01:52,600 --> 00:01:55,250
This method is actually much more powerful than we think.

32
00:01:55,250 --> 00:01:59,180
If we calculate the probabilities and pair the points with the corresponding logarithms,

33
00:01:59,180 --> 00:02:01,470
we actually get an error for each point.

34
00:02:01,470 --> 00:02:06,540
So again, here we have probabilities for both models and the products of them.

35
00:02:06,540 --> 00:02:09,945
Now, we take the negative of the logarithms which gives us sum of

36
00:02:09,945 --> 00:02:15,320
logarithms and if we pair each logarithm with the point where it came from,

37
00:02:15,320 --> 00:02:17,860
we actually get a value for each point.

38
00:02:17,860 --> 00:02:19,565
And if we calculate the values,

39
00:02:19,565 --> 00:02:22,185
we get this. Check it out.

40
00:02:22,185 --> 00:02:24,320
If we look carefully at the values we can see that

41
00:02:24,320 --> 00:02:26,430
the points that are mis-classified has like

42
00:02:26,430 --> 00:02:31,295
values like 2.3 for this point or 1.6 one for this point,

43
00:02:31,295 --> 00:02:36,545
whereas the points that are correctly classified have small values.

44
00:02:36,545 --> 00:02:38,720
And the reason for this is again is that

45
00:02:38,720 --> 00:02:42,605
a correctly classified point will have a probability that as close to 1,

46
00:02:42,605 --> 00:02:44,990
which when we take the negative of the logarithm,

47
00:02:44,990 --> 00:02:46,915
we'll get a small value.

48
00:02:46,915 --> 00:02:51,215
Thus we can think of the negatives of these logarithms as errors at each point.

49
00:02:51,215 --> 00:02:53,540
Points that are correctly classified will have

50
00:02:53,540 --> 00:02:57,595
small errors and points that are mis-classified will have large errors.

51
00:02:57,595 --> 00:03:02,530
And now we've concluded that our cross entropy will tell us if a model is good or bad.

52
00:03:02,530 --> 00:03:06,800
So now our goal has changed from maximizing a probability to minimizing

53
00:03:06,800 --> 00:03:12,580
a cross entropy in order to get from the model in left to the model in the right.

54
00:03:12,580 --> 00:03:14,655
And that error function that we're looking for,

55
00:03:14,655 --> 00:03:17,000
that was precisely the cross entropy.
